{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monolingual Norec experiments with bert\n",
    "Targeted sentiment analysis with simpletransformers for sequence tagging. Highly recommend to do this in a dedicated Python environment. You need PyTorch to interact with Cuda, and Simpletransformers to interact with pytorch, and you need the right Python version to support this chain. I suggest you begin with having a Cuda version that is listed in the pytorch installation guide, and take it from there.  \n",
    "\n",
    "### CPU only.\n",
    "For exploring norBERT with SimpleTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda: False\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import time, datetime\n",
    "random.seed(64)\n",
    "# import nltk\n",
    "import re\n",
    "# from nltk.tokenize.simple import SpaceTokenizer\n",
    "from helpers import *\n",
    "\n",
    "print(f\"Cuda: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data.\n",
    "I collect the experiments to run as a list of tuples. Each experiment consists of training set and testing set, and a list of tags. This list of tags is used only for checking that the files have the tags we expect. Highly recommend to do this checking, that we have the expected tags in the datasets. Token and tags need to be spaceseparated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags checked OK\n"
     ]
    }
   ],
   "source": [
    "datasets = [(\"data/spaceseparated/norec_fine/norec_fine_train.conll\", 'data/spaceseparated/norec_fine/norec_fine_dev.conll', ['I-targ-negative', 'I-targ-positive',\n",
    " 'B-targ-negative', 'B-targ-positive', 'O'])]\n",
    "'''\n",
    "datasets = [\n",
    " ( \"data/spaceseparated/norec_fine_nopolarity/nopol_norec_fine_train.conll\"\n",
    " , \"data/spaceseparated/norec_fine_nopolarity/nopol_norec_fine_dev.conll\",\n",
    "  ['I-targ', 'B-targ',  'O'] )] # To not repeat the first which was successful\n",
    "'''\n",
    "for train_path, dev_path, tags in datasets:\n",
    "    for path in [dev_path,train_path]:\n",
    "        with open (path) as rf:\n",
    "            text = rf.read()\n",
    "            # print(path)\n",
    "            # print(tags)\n",
    "            assert tagsset(text, separator = \" \") == set(tags), tagsset(text, separator = \" \")\n",
    "print(\"Tags checked OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the fine-tuning\n",
    "Note that this script does not automatically  create the folders needed to save the model and to record the output. I recommend you run the following cell with 1 epoch to see that this works, befor setting it back to 8 or whatever you consider to be adequate. 3 should be enough, but I got a litte better result with 8 so I kept that.\n",
    "\n",
    "Simpletransformers has included the code for wandb but I have not tried to connect and use that what is supposed to be a great reporting and logging tool.\n",
    "\n",
    "Note that if you run many epochs and save the models, you will need a lot of space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'use_cuda' set to True when cuda is unavailable.Make sure CUDA is available or set use_cuda=False.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m model_args\u001b[38;5;241m.\u001b[39msave_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200000\u001b[39m\n\u001b[1;32m     15\u001b[0m model_args\u001b[38;5;241m.\u001b[39muse_cuda \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mNERModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtransformersmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m out_d \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs/simpletransformers/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtransformersmodel\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtrain_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     20\u001b[0m running \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(out_d, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Logging individual results\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/simplet/lib/python3.9/site-packages/simpletransformers/ner/ner_model.py:287\u001b[0m, in \u001b[0;36mNERModel.__init__\u001b[0;34m(self, model_type, model_name, labels, weight, args, use_cuda, cuda_device, onnx_execution_provider, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcuda_device\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 287\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    288\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_cuda\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m set to True when cuda is unavailable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    289\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure CUDA is available or set use_cuda=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         )\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: 'use_cuda' set to True when cuda is unavailable.Make sure CUDA is available or set use_cuda=False."
     ]
    }
   ],
   "source": [
    "# Run bert multilingual with the data from previous cell\n",
    "\n",
    "family = \"bert\"\n",
    "transformersmodel = 'bert-base-multilingual-cased'\n",
    "results = []\n",
    "\n",
    "for train_path, dev_path, tags in datasets:\n",
    "    model_args = NERArgs() # New args loading fall 2020\n",
    "    model_args.train_batch_size = 32\n",
    "    model_args.num_train_epochs = 1\n",
    "    model_args.weight_decay = 0.001\n",
    "    model_args.overwrite_output_dir = True\n",
    "    model_args.silent = False\n",
    "    model_args.save_steps = 200000\n",
    "\n",
    "    model = NERModel(family,transformersmodel , labels = tags,args=model_args, use_cuda=False)\n",
    "\n",
    "    out_d = \"outputs/simpletransformers/\"+transformersmodel+\"_\"+train_path.split(\"/\")[-2]\n",
    "    running = os.path.join(out_d, \"running\") # Logging individual results\n",
    "    if not os.path.exists(os.path.dirname(running)):\n",
    "        os.makedirs(os.path.dirname(running))\n",
    "\n",
    "\n",
    "    model.train_model(train_path, output_dir= out_d)\n",
    "    print(transformersmodel, \"Done training\")\n",
    "\n",
    "    result, model_outputs, predictions = model.eval_model(dev_path)\n",
    "\n",
    "    #Record settings and results\n",
    "    result[\"train\"] = train_path\n",
    "    result[\"dev_test\"] = dev_path\n",
    "    result[\"training_epochs\"] = model_args.num_train_epochs\n",
    "    result[\"transformer_model\"] = transformersmodel\n",
    "    json_path = os.path.join(running,\"result_\"+datetime.datetime.now().strftime(\"%Y%m%d%H%M\")+\".json\")\n",
    "    if not os.path.exists(os.path.dirname(json_path)):\n",
    "        os.makedirs(os.path.dirname(json_path))\n",
    "    with open(json_path, \"w\") as wf:\n",
    "        json.dump(result, wf)\n",
    "    results.append(result)\n",
    "    with open(os.path.join(running, \"norec_fine_mono_predictions\" + datetime.datetime.now().strftime(\"%Y%m%d%H%M\")+\".json\" ), \"w\") as wf:\n",
    "        json.dump(predictions, wf)\n",
    "\n",
    "json_path = \"summaries/results_\"+datetime.datetime.now().strftime(\"%Y%m%d%H%M\")+\".json\"\n",
    "if not os.path.exists(os.path.dirname(json_path)):\n",
    "    os.makedirs(os.path.dirname(json_path))\n",
    "with open(json_path, \"w\") as wf:\n",
    "    json.dump(results, wf)\n",
    "df = pd.DataFrame.from_dict(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>train</th>\n",
       "      <th>dev_test</th>\n",
       "      <th>training_epochs</th>\n",
       "      <th>transformer_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.322868</td>\n",
       "      <td>0.369099</td>\n",
       "      <td>0.392246</td>\n",
       "      <td>0.380321</td>\n",
       "      <td>data/spaceseparated/norec_fine/norec_fine_trai...</td>\n",
       "      <td>data/spaceseparated/norec_fine/norec_fine_dev....</td>\n",
       "      <td>8</td>\n",
       "      <td>bert-base-multilingual-cased</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eval_loss  precision    recall  f1_score  \\\n",
       "0   0.322868   0.369099  0.392246  0.380321   \n",
       "\n",
       "                                               train  \\\n",
       "0  data/spaceseparated/norec_fine/norec_fine_trai...   \n",
       "\n",
       "                                            dev_test  training_epochs  \\\n",
       "0  data/spaceseparated/norec_fine/norec_fine_dev....                8   \n",
       "\n",
       "              transformer_model  \n",
       "0  bert-base-multilingual-cased  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run all above\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"summaries/norec_mbert.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inference\n",
    "First on the existing model, then an example of how to load a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 49.36it/s]\n",
      "Running Prediction: 100%|██████████| 1/1 [00:00<00:00, 29.12it/s][{'Mannen': 'B-targ-negative'}, {'på': 'O'}, {'scenen': 'O'}, {'synger': 'O'}, {'stygt': 'O'}]\n",
      "[{'Damen': 'B-targ-positive'}, {'på': 'I-targ-positive'}, {'scenen': 'I-targ-positive'}, {'synger': 'O'}, {'stygt': 'O'}]\n",
      "[{'Disse': 'O'}, {'bilene': 'I-targ-positive'}, {'har': 'O'}, {'et': 'O'}, {'fantastisk': 'O'}, {'veigrep': 'O'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions, raw_outputs = model.predict ([\"Mannen på scenen synger stygt\", \"Damen på scenen synger stygt\" , \"Disse bilene har et fantastisk veigrep\"])\n",
    "for sentence in predictions:\n",
    "    print(sentence)\n",
    "    # print(json.dumps(sentence, indent=3, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_d: 'outputs/simpletransformers/bert-base-multilingual-cased_norec_fine'\n",
    "last_epoch= sorted([f for f in os.listdir(out_d) if \"-\" in f], key = lambda x: int(x.split(\"-\")[-1]) )[-1]\n",
    "print(last_epoch)\n",
    "model2 = NERModel(family, os.path.join(out_d, last_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 46.34it/s]\n",
      "Running Prediction: 100%|██████████| 1/1 [00:00<00:00, 34.20it/s][{'Mannen': 'B-targ-negative'}, {'på': 'O'}, {'scenen': 'O'}, {'synger': 'O'}, {'stygt': 'O'}]\n",
      "[{'Damen': 'B-targ-positive'}, {'på': 'I-targ-positive'}, {'scenen': 'I-targ-positive'}, {'synger': 'O'}, {'stygt': 'O'}]\n",
      "[{'Disse': 'O'}, {'bilene': 'I-targ-positive'}, {'har': 'O'}, {'et': 'O'}, {'fantastisk': 'O'}, {'veigrep': 'O'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions, raw_outputs = model2.predict ([\"Mannen på scenen synger stygt\", \"Damen på scenen synger stygt\" , \"Disse bilene har et fantastisk veigrep\"])\n",
    "for sentence in predictions:\n",
    "    print(sentence)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c3d8fa28104af9453ca10de36764252d849c37850bd5d27fab086321a244c9e6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('simplet': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
