{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monolingual Norec experiments with bert\n",
    "Targeted sentiment analysis with simpletransformers for sequence tagging. Highly recommend to do this in a dedicated Python environment. You need PyTorch to interact with Cuda, and Simpletransformers to interact with pytorch, and you need the right Python version to support this chain. I suggest you begin with having a Cuda version that is listed in the pytorch installation guide, and take it from there.  \n",
    "\n",
    "### GPU Clean installsimpletransformers.\n",
    "For exploring norBERT with SimpleTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda: True\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import time, datetime\n",
    "random.seed(64)\n",
    "# import nltk\n",
    "import re\n",
    "# from nltk.tokenize.simple import SpaceTokenizer\n",
    "from helpers import *\n",
    "\n",
    "print(f\"Cuda: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data.\n",
    "I collect the experiments to run as a list of tuples. Each experiment consists of training set and testing set, and a list of tags. This list of tags is used only for checking that the files have the tags we expect. Highly recommend to do this checking, that we have the expected tags in the datasets. Token and tags need to be spaceseparated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags checked OK\n"
     ]
    }
   ],
   "source": [
    "# Origianlly made for multiple datasets\n",
    "datasets = [('data/spaceseparated/norec_fine/norec_fine_train.conll', 'data/spaceseparated/norec_fine/norec_fine_dev.conll', ['I-targ-negative', 'I-targ-positive',\n",
    " 'B-targ-negative', 'B-targ-positive', 'O'])]\n",
    "train_path, dev_path, tags = datasets[0]\n",
    "# Check that the tags are consistent \n",
    "for path in [dev_path,train_path]:\n",
    "    with open (path) as rf:\n",
    "        text = rf.read()\n",
    "        assert tagsset(text, separator = \" \") == set(tags), tagsset(text, separator = \" \")\n",
    "print(\"Tags checked OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the fine-tuning\n",
    "Note that this script does not automatically  create the folders needed to save the model and to record the output. I recommend you run the following cell with 1 epoch to see that this works, befor setting it back to 8 or whatever you consider to be adequate. 3 should be enough, but I got a litte better result with 8 so I kept that.\n",
    "\n",
    "Simpletransformers has included the code for wandb but I have not tried to connect and use that what is supposed to be a great reporting and logging tool.\n",
    "\n",
    "Note that if you run many epochs and save the models, you will need a lot of space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 18/18 [00:04<00:00,  3.72it/s]\n",
      "Epoch 1 of 3:   0%|          | 0/3 [00:00<?, ?it/s]/home/egil/miniconda3/envs/st_gpu/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Epochs 0/3. Running Loss:    0.4786: 100%|██████████| 720/720 [04:27<00:00,  2.69it/s]\n",
      "Epochs 1/3. Running Loss:    0.0309: 100%|██████████| 720/720 [04:23<00:00,  2.73it/s]\n",
      "Epochs 2/3. Running Loss:    0.0294: 100%|██████████| 720/720 [04:23<00:00,  2.73it/s]\n",
      "Epoch 3 of 3: 100%|██████████| 3/3 [13:14<00:00, 264.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-multilingual-cased Done training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  4.38it/s]\n",
      "Running Evaluation: 100%|██████████| 192/192 [00:12<00:00, 15.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-multilingual-cased eval_loss 0.2119008700331809\n",
      "bert-base-multilingual-cased precision 0.37882096069869\n",
      "bert-base-multilingual-cased recall 0.3956670467502851\n",
      "bert-base-multilingual-cased f1_score 0.38706079196876747\n",
      "bert-base-multilingual-cased transformer_model bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 15.46it/s]\n",
      "Running Prediction: 100%|██████████| 1/1 [00:00<00:00, 33.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Mannen': 'B-targ-negative'}, {'på': 'O'}, {'scenen': 'I-targ-negative'}, {'synger': 'O'}, {'stygt': 'O'}]\n",
      "[{'Damen': 'B-targ-positive'}, {'på': 'I-targ-positive'}, {'scenen': 'I-targ-negative'}, {'synger': 'O'}, {'stygt': 'O'}]\n",
      "[{'Disse': 'O'}, {'bilene': 'B-targ-positive'}, {'har': 'O'}, {'et': 'O'}, {'fantastisk': 'O'}, {'veigrep': 'O'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at NbAiLab/nb-bert-base were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 18/18 [00:05<00:00,  3.30it/s]\n",
      "Epochs 0/3. Running Loss:    0.1656: 100%|██████████| 720/720 [04:19<00:00,  2.77it/s]\n",
      "Epochs 1/3. Running Loss:    0.0186: 100%|██████████| 720/720 [04:20<00:00,  2.76it/s]\n",
      "Epochs 2/3. Running Loss:    0.0072: 100%|██████████| 720/720 [04:22<00:00,  2.75it/s]\n",
      "Epoch 3 of 3: 100%|██████████| 3/3 [13:03<00:00, 261.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NbAiLab/nb-bert-base Done training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  4.73it/s]\n",
      "Running Evaluation: 100%|██████████| 192/192 [00:12<00:00, 15.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NbAiLab/nb-bert-base eval_loss 0.1838499246837273\n",
      "NbAiLab/nb-bert-base precision 0.5256869772998806\n",
      "NbAiLab/nb-bert-base recall 0.5017103762827823\n",
      "NbAiLab/nb-bert-base f1_score 0.5134189031505252\n",
      "NbAiLab/nb-bert-base transformer_model NbAiLab/nb-bert-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.64it/s]\n",
      "Running Prediction: 100%|██████████| 1/1 [00:00<00:00, 32.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Mannen': 'B-targ-negative'}, {'på': 'O'}, {'scenen': 'O'}, {'synger': 'O'}, {'stygt': 'O'}]\n",
      "[{'Damen': 'B-targ-negative'}, {'på': 'O'}, {'scenen': 'O'}, {'synger': 'O'}, {'stygt': 'O'}]\n",
      "[{'Disse': 'B-targ-positive'}, {'bilene': 'I-targ-positive'}, {'har': 'O'}, {'et': 'O'}, {'fantastisk': 'O'}, {'veigrep': 'O'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltgoslo/norbert were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ltgoslo/norbert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 18/18 [00:04<00:00,  4.01it/s]\n",
      "Epochs 0/3. Running Loss:       nan: 100%|██████████| 720/720 [03:13<00:00,  3.72it/s]\n",
      "Epochs 1/3. Running Loss:       nan: 100%|██████████| 720/720 [03:09<00:00,  3.79it/s]\n",
      "Epochs 2/3. Running Loss:       nan: 100%|██████████| 720/720 [03:08<00:00,  3.82it/s]\n",
      "Epoch 3 of 3: 100%|██████████| 3/3 [09:31<00:00, 190.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ltgoslo/norbert Done training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.83it/s]\n",
      "Running Evaluation: 100%|██████████| 192/192 [00:11<00:00, 16.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ltgoslo/norbert eval_loss nan\n",
      "ltgoslo/norbert precision 0.0\n",
      "ltgoslo/norbert recall 0.0\n",
      "ltgoslo/norbert f1_score 0.0\n",
      "ltgoslo/norbert transformer_model ltgoslo/norbert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 61.25it/s]\n",
      "Running Prediction: 100%|██████████| 1/1 [00:00<00:00, 35.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Mannen': 'I-targ-negative'}, {'på': 'I-targ-negative'}, {'scenen': 'I-targ-negative'}, {'synger': 'I-targ-negative'}, {'stygt': 'I-targ-negative'}]\n",
      "[{'Damen': 'I-targ-negative'}, {'på': 'I-targ-negative'}, {'scenen': 'I-targ-negative'}, {'synger': 'I-targ-negative'}, {'stygt': 'I-targ-negative'}]\n",
      "[{'Disse': 'I-targ-negative'}, {'bilene': 'I-targ-negative'}, {'har': 'I-targ-negative'}, {'et': 'I-targ-negative'}, {'fantastisk': 'I-targ-negative'}, {'veigrep': 'I-targ-negative'}]\n"
     ]
    }
   ],
   "source": [
    "# Run bert multilingual with the data from previous cell\n",
    "\n",
    "family = \"bert\"\n",
    "transformersmodels = ['bert-base-multilingual-cased' , 'NbAiLab/nb-bert-base',  'ltgoslo/norbert' ]\n",
    "results = []\n",
    "\n",
    "for transformersmodel in transformersmodels:\n",
    "    model_args = NERArgs() \n",
    "    model_args.train_batch_size = 12\n",
    "    model_args.num_train_epochs = 3\n",
    "    model_args.weight_decay = 0.001\n",
    "    model_args.overwrite_output_dir = True\n",
    "    model_args.silent = False\n",
    "    model_args.save_model_every_epoch = False\n",
    "    model_args.save_eval_checkpoints = True\n",
    "    model_args.save_steps = -1\n",
    "    model_args.warmup_ratio = 0\n",
    "\n",
    "    model = NERModel(family,transformersmodel , labels = tags,args=model_args)\n",
    "\n",
    "    out_d = \"outputs/simpletransformers/\"+transformersmodel+\"_\"+train_path.split(\"/\")[-2]\n",
    "\n",
    "    model.train_model(train_path, output_dir= out_d)\n",
    "    print(transformersmodel, \"Done training\")\n",
    "\n",
    "    result, model_outputs, predictions = model.eval_model(dev_path)\n",
    "    result[\"transformer_model\"] = transformersmodel\n",
    "    results.append(result)\n",
    "    for k, v in result.items():\n",
    "        print(transformersmodel, k, v)\n",
    "    predictions, raw_outputs = model.predict ([\"Mannen på scenen synger stygt\", \"Damen på scenen synger stygt\" , \"Disse bilene har et fantastisk veigrep\"])\n",
    "    for sentence in predictions:\n",
    "        print(sentence)\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_dict(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>transformer_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.211901</td>\n",
       "      <td>0.378821</td>\n",
       "      <td>0.395667</td>\n",
       "      <td>0.387061</td>\n",
       "      <td>bert-base-multilingual-cased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.183850</td>\n",
       "      <td>0.525687</td>\n",
       "      <td>0.501710</td>\n",
       "      <td>0.513419</td>\n",
       "      <td>NbAiLab/nb-bert-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ltgoslo/norbert</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eval_loss  precision    recall  f1_score             transformer_model\n",
       "0   0.211901   0.378821  0.395667  0.387061  bert-base-multilingual-cased\n",
       "1   0.183850   0.525687  0.501710  0.513419          NbAiLab/nb-bert-base\n",
       "2        NaN   0.000000  0.000000  0.000000               ltgoslo/norbert"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run all above\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"summaries/norec_3berts_gpu.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c3d8fa28104af9453ca10de36764252d849c37850bd5d27fab086321a244c9e6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('simplet': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
