{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monolingual Norec experiments with bert\n",
    "These were originally in the Experiments2 notebook. I have not run them since I moved them here, so maybe there is an undefined variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cuda: True\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import time, datetime\n",
    "random.seed(64)\n",
    "# import nltk\n",
    "import re\n",
    "# from nltk.tokenize.simple import SpaceTokenizer\n",
    "from helpers import *\n",
    "from keep_records import Keep_records\n",
    "\n",
    "print(f\"Cuda: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['B-MISC', 'B-PER', 'B-ORG', 'I-LOC', 'I-ORG', 'O', 'B-LOC', 'I-PER', 'I-MISC']\n"
     ]
    }
   ],
   "source": [
    "datafolder = \"norne/data/\"\n",
    "train_all = datafolder + \"no_bokmaal-ud-train.bmes\"\n",
    "dev_all = datafolder + 'no_bokmaal-ud-dev.bmes'\n",
    "train_4c = datafolder + \"no_bokmaal-ud-train_4c.bmes\"\n",
    "dev_4c = datafolder + 'no_bokmaal-ud-dev_4c.bmes'\n",
    "\n",
    "dataset_4c = dataset_w_tags(train_4c, dev_4c)\n",
    "dataset_all = dataset_w_tags(train_all, dev_all)\n",
    "datasets = [dataset_4c, dataset_all]\n",
    "keeper = Keep_records()\n",
    "for ds in datasets:\n",
    "    print(ds[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " ",
      "results_1059.json\n"
     ]
    }
   ],
   "source": [
    "# Run bert multilingual with the settings from previous cell\n",
    "\n",
    "# family = \"bert\"\n",
    "family = \"xlmroberta\"\n",
    "# transformersmodel = 'bert-base-multilingual-cased'\n",
    "transformersmodel = 'xlm-roberta-base'\n",
    "results = []\n",
    "\n",
    "for train_path, dev_path, tags in datasets:\n",
    "    model_args = NERArgs() # New args loading fall 2020\n",
    "    model_args.train_batch_size = 16\n",
    "    model_args.num_train_epochs = 8\n",
    "    model_args.weight_decay = 0.001\n",
    "    model_args.overwrite_output_dir = True\n",
    "    model_args.silent = True\n",
    "    model_args.save_steps = -1\n",
    "    model_args.evaluate_during_training = True\n",
    "\n",
    "    model = NERModel(family, transformersmodel, labels = tags, args=model_args)\n",
    "\n",
    "    out_d = \"outputs/\"+transformersmodel+\"_\"+keeper.ts\n",
    "    model.train_model(train_path, output_dir= out_d, eval_data=dev_path)\n",
    "    print(transformersmodel, \"Done training\")\n",
    "\n",
    "    result, model_outputs, predictions = model.eval_model(dev_path)\n",
    "\n",
    "    #Record settings and results\n",
    "    result[\"train\"] = train_path\n",
    "    result[\"dev_test\"] = dev_path\n",
    "    result[\"training_epochs\"] = model_args.num_train_epochs\n",
    "    result[\"transformer_model\"] = transformersmodel\n",
    "    keeper.keep(result)\n",
    "\n",
    "df = keeper.get_df()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   eval_loss  precision    recall  f1_score  \\\n",
       "0   0.017232   0.919859  0.924763  0.922304   \n",
       "\n",
       "                                    train  \\\n",
       "0  norne/data/no_bokmaal-ud-train_4c.bmes   \n",
       "\n",
       "                               dev_test  training_epochs transformer_model  \\\n",
       "0  norne/data/no_bokmaal-ud-dev_4c.bmes                1  xlm-roberta-base   \n",
       "\n",
       "  timestamp  \n",
       "0    111005  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eval_loss</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1_score</th>\n      <th>train</th>\n      <th>dev_test</th>\n      <th>training_epochs</th>\n      <th>transformer_model</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.017232</td>\n      <td>0.919859</td>\n      <td>0.924763</td>\n      <td>0.922304</td>\n      <td>norne/data/no_bokmaal-ud-train_4c.bmes</td>\n      <td>norne/data/no_bokmaal-ud-dev_4c.bmes</td>\n      <td>1</td>\n      <td>xlm-roberta-base</td>\n      <td>111005</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Run all above\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keeper.to_csv(df.round(3))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bitsimpletcondaa9e043320cae4e98a1b239d0bee7ff2a",
   "display_name": "Python 3.7.9 64-bit ('simplet': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}